{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfoFactAgents User Interface\n",
    "This Jupyter Notebook implements a web user interface to InfoFactAgents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio requests --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'with' statement on line 100 (2767238709.py, line 103)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 103\u001b[1;36m\u001b[0m\n\u001b[1;33m    with gr.Row(equal_height=True):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'with' statement on line 100\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# JavaScript function to enforce the dark theme\n",
    "js_func = \"\"\"\n",
    "function refresh() {\n",
    "    const url = new URL(window.location);\n",
    "\n",
    "    if (url.searchParams.get('__theme') !== 'dark') {\n",
    "        url.searchParams.set('__theme', 'dark');\n",
    "        window.location.href = url.href;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Read authentication credentials from environment variables\n",
    "USERNAME = os.environ.get(\"INFOFACT_USERNAME\")\n",
    "PASSWORD = os.environ.get(\"INFOFACT_PASSWORD\")\n",
    "\n",
    "def authenticate(username, password):\n",
    "    \"\"\"Simple authentication function.\"\"\"\n",
    "    return username == USERNAME and password == PASSWORD\n",
    "\n",
    "def logout(request: gr.Request):\n",
    "    \"\"\"Logout handler.\"\"\"\n",
    "    print(\"User logged out.\")\n",
    "\n",
    "def append_debug_message(debug_messages, message):\n",
    "    \"\"\"Appends a timestamped debug message to the debug log.\"\"\"\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "    debug_messages.append(f\"{timestamp} D {message}\")\n",
    "    return debug_messages\n",
    "\n",
    "def fetch_agents(backend_url, session_id, debug_messages):\n",
    "    \"\"\"Fetches the list of agents from the backend and logs the communication.\"\"\"\n",
    "    try:\n",
    "        debug_messages = append_debug_message(debug_messages, f\"Sending request to fetch agents: {backend_url}\")\n",
    "        response = requests.post(backend_url, data={\"Body\": \"list_agents\", \"SessionID\": session_id})\n",
    "        if response.status_code == 200:\n",
    "            response_data = response.json()\n",
    "            session_id = response_data.get(\"SessionID\", session_id)  # Update session ID\n",
    "            agents_text = response_data.get(\"Response\", \"\")\n",
    "            agent_descriptions = [line for line in agents_text.split(\"\\n\") if \" - \" in line]\n",
    "            debug_messages = append_debug_message(debug_messages, f\"Received agents: {agent_descriptions}\")\n",
    "            return agent_descriptions, session_id, debug_messages\n",
    "        else:\n",
    "            error_message = f\"Error: Unable to fetch agents. Status code: {response.status_code}\"\n",
    "            debug_messages = append_debug_message(debug_messages, error_message)\n",
    "            return [error_message], session_id, debug_messages\n",
    "    except requests.RequestException as e:\n",
    "        error_message = f\"Request failed: {e}\"\n",
    "        debug_messages = append_debug_message(debug_messages, error_message)\n",
    "        return [error_message], session_id, debug_messages\n",
    "\n",
    "def analyze(prompt, backend_url, selected_agents, session_id, debug_messages):\n",
    "    \"\"\"Sends the article or URL to the backend for analysis and logs the communication.\"\"\"\n",
    "    try:\n",
    "        from urllib.parse import urlparse\n",
    "        parsed_url = urlparse(prompt)\n",
    "        if parsed_url.scheme in [\"http\", \"https\"]:\n",
    "            debug_messages = append_debug_message(debug_messages, f\"Fetching article content from URL: {prompt}\")\n",
    "            try:\n",
    "                response = requests.get(prompt)\n",
    "                response.raise_for_status()\n",
    "                prompt = response.text\n",
    "                debug_messages = append_debug_message(debug_messages, \"Successfully fetched article content.\")\n",
    "            except requests.RequestException as e:\n",
    "                error_message = f\"Error fetching article content: {e}\"\n",
    "                debug_messages = append_debug_message(debug_messages, error_message)\n",
    "                return error_message, session_id, debug_messages\n",
    "\n",
    "        selected_agents = [agent.split(\" - \")[0] for agent in selected_agents if agent]\n",
    "        debug_messages = append_debug_message(debug_messages, f\"Sending analysis request with agents: {selected_agents}\")\n",
    "        response = requests.post(\n",
    "            backend_url,\n",
    "            data={\"Body\": prompt, \"Agents\": \",\".join(selected_agents), \"SessionID\": session_id},\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            response_data = response.json()\n",
    "            session_id = response_data.get(\"SessionID\", session_id)\n",
    "            response_text = response_data.get(\"Response\", \"\")\n",
    "            debug_messages = append_debug_message(debug_messages, f\"Received analysis response: {response_text}\")\n",
    "            return response_text, session_id, debug_messages\n",
    "        else:\n",
    "            error_message = f\"Error: {response.status_code} - {response.text}\"\n",
    "            debug_messages = append_debug_message(debug_messages, error_message)\n",
    "            return error_message, session_id, debug_messages\n",
    "    except requests.RequestException as e:\n",
    "        error_message = f\"Request failed: {e}\"\n",
    "        debug_messages = append_debug_message(debug_messages, error_message)\n",
    "        return error_message, session_id, debug_messages\n",
    "\n",
    "with gr.Blocks(js=js_func) as interface:\n",
    "    debug_messages = gr.State([])  # State to store debug messages\n",
    "    session_id = gr.State(\"\")  # State to store the SessionID\n",
    "\n",
    "    with gr.Tab(\"Article\"):\n",
    "\n",
    "        # Display flow.jpg and a text box with project information side by side\n",
    "        with gr.Row(equal_height=True):\n",
    "            with gr.Column(scale=2):\n",
    "                gr.Textbox(value=\"Welcome to the InfoFactAgents Fact-Checking Tool!\\n\\nThis tool is designed to analyze the trustworthiness of news articles and other textual content. It leverages advanced AI agents and external tools for factual consistency, metadata analysis, and sentiment evaluation.\\n\\nThis exploratory tool was a byproduct of Medialukutaito Infotester -coursework in Tampere University of Applied Sciences.\\n\\nProject Team:\\n- Essi Salonen\\n- Mika Wilen\\n- Joona Sjöholm\\n- Jasperi Järveläinen\\n\\nVisit the GitHub repository at https://github.com/mwtuni/infofactagents to explore the codebase and contribute!\", label=\"Project Info\", interactive=False)\n",
    "            with gr.Column(scale=1):\n",
    "                gr.Image(\"flow.jpg\", label=\"Workflow Overview\", elem_id=\"workflow-image\")\n",
    "\n",
    "        # Add custom CSS\n",
    "        gr.HTML(\"\"\"\n",
    "        <style>\n",
    "            #workflow-image img {\n",
    "                height: 100%;\n",
    "                object-fit: contain;\n",
    "            }\n",
    "        </style>\n",
    "        \"\"\")\n",
    "\n",
    "        # Backend Section\n",
    "        with gr.Row():\n",
    "            backend_url = gr.Textbox(label=\"Backend URL\", value=\"http://localhost:5000/infofactagents\", interactive=True)\n",
    "\n",
    "        # Agents Section\n",
    "        with gr.Row():\n",
    "            enabled_agents = gr.CheckboxGroup(label=\"Enable Agents\", choices=[], interactive=True)\n",
    "\n",
    "        # Article Section\n",
    "        with gr.Row():\n",
    "            prompt = gr.Textbox(label=\"Enter news article\", lines=5, interactive=True)\n",
    "\n",
    "        # Analyze Button and Trustworthiness Output\n",
    "        with gr.Row():\n",
    "            send_button = gr.Button(\"Analyze\")\n",
    "        with gr.Row():\n",
    "            output = gr.Textbox(label=\"Trustworthiness Analysis\", lines=10, interactive=False)\n",
    "\n",
    "        def update_agents(backend_url, session_id, debug_messages):\n",
    "            agents, session_id, updated_debug = fetch_agents(backend_url, session_id, debug_messages)\n",
    "            return gr.update(choices=agents), session_id, updated_debug\n",
    "\n",
    "        # Automatically fetch agents when the interface starts\n",
    "        interface.load(\n",
    "            fn=update_agents,\n",
    "            inputs=[backend_url, session_id, debug_messages],\n",
    "            outputs=[enabled_agents, session_id, debug_messages],\n",
    "        )\n",
    "\n",
    "        send_button.click(\n",
    "            fn=analyze,\n",
    "            inputs=[prompt, backend_url, enabled_agents, session_id, debug_messages],\n",
    "            outputs=[output, session_id, debug_messages],\n",
    "        )\n",
    "\n",
    "    with gr.Tab(\"Debug\") as debug_tab:\n",
    "        debug_output = gr.Textbox(label=\"Debug Info\", lines=15, interactive=False)\n",
    "\n",
    "        # Refresh debug info whenever the Debug tab is activated\n",
    "        debug_tab.select(\n",
    "            fn=lambda debug_messages: \"\\n\".join(debug_messages),\n",
    "            inputs=debug_messages,\n",
    "            outputs=debug_output,\n",
    "        )\n",
    "\n",
    "    # Logout Button\n",
    "    logout_button = gr.Button(value=\"Logout\")\n",
    "    logout_button.click(logout)\n",
    "\n",
    "    # Automatically refresh the page to enforce the dark theme\n",
    "    gr.HTML(\"<script>refresh();</script>\")\n",
    "\n",
    "interface.launch(auth=authenticate, share=False, server_port=5042, server_name=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display flow.jpg and a text box with project information side by side\n",
    "with gr.Row(equal_height=True):\n",
    "    with gr.Column(scale=2):\n",
    "        gr.Textbox(value=\"Welcome to the InfoFactAgents Fact-Checking Tool!\\n\\nThis tool is designed to analyze the trustworthiness of news articles and other textual content. It leverages advanced AI agents and external tools for factual consistency, metadata analysis, and sentiment evaluation.\\n\\nThis exploratory tool was a byproduct of Medialukutaito Infotester -coursework in Tampere University of Applied Sciences.\\n\\nProject Team:\\n- Essi Salonen\\n- Mika Wilen\\n- Joona Sjöholm\\n- Jasperi Järveläinen\\n\\nVisit the GitHub repository at https://github.com/mwtuni/infofactagents to explore the codebase and contribute!\", label=\"Project Info\", interactive=False)\n",
    "    with gr.Column(scale=1):\n",
    "        gr.Image(\"flow.jpg\", label=\"Workflow Overview\", scale=0.3, style={\"height\": \"100%\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display flow.jpg and a text box with project information side by side\n",
    "with gr.Row(equal_height=True):\n",
    "    with gr.Column(scale=2):\n",
    "        gr.Textbox(value=\"Welcome to the InfoFactAgents Fact-Checking Tool!\\n\\nThis tool is designed to analyze the trustworthiness of news articles and other textual content. It leverages advanced AI agents and external tools for factual consistency, metadata analysis, and sentiment evaluation.\\n\\nThis tool was a byproduct of Medialukutaito Infotester -coursework in Tampere University of Applied Sciences.\\n\\nProject Team:\\n- Essi Salonen\\n- Mika Wilen\\n- Joona Sjöholm\\n- Jasperi Järveläinen\\n\\nWe welcome contributions to improve and expand this tool. Visit the [GitHub repository](https://github.com/mwtuni/infofactagents) to explore the codebase and contribute!\", label=\"Project Info\", interactive=False)\n",
    "    with gr.Column(scale=1):\n",
    "        gr.Image(\"flow.jpg\", label=\"Workflow Overview\", scale=0.3)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "sihteeri_gradio.ipynb"
  },
  "kernelspec": {
   "display_name": "mwtuni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
